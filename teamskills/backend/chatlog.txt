üß± 9. SkillSync: AI Agents for Team Dynamics
üí° Core Idea

A tool that analyzes a team‚Äôs profiles (LinkedIn, resumes, GitHub) and suggests optimal task distribution or team formation strategies.

üß† Agents

Skill Agent: Extracts technical expertise.

Personality Agent: Evaluates working style.

Match Agent: Pairs complementary members.

Planner Agent: Suggests who should lead what task.

üí° Innovation

AI for team optimization ‚Äî great for organizations or hackathons themselves.

for this hackathon project idea, go into depth about project specifications and how it can be done. 
You said:
for 4 people, give a detailed breakdown of tasks that we can do. ensure that all of these tasks are parallelizable and can be done at the same time
You said:
I'm thinking of using an MCP server with google-adk to trigger backend functions to do these functionalities. Is there a better way to implement this, perhaps a faster way to do it for a hackathon? I'm thinking the main language we'll use is python since there are ML components, but suggest me a quick way to do this. I'm also considering a React and Next.js frontend.
You said:
Before you do that, I want to ask about the tools we should use in this app. Do you think we should use a chatbot to input project idea, github username, and resume PDF? Or should we do this in an app format? What would look better on a hackathon? I know a standalone chatbot may not look too awesome on its own and a website could probably look better.
You said:
If my whole app uses Agentic AI and is a chatbot itself, withou the tangible file uploads and short-answer text submission form for project idea, then would that look good? Since it's using Agentic AI, it feels quite appealing to me. But would that look thin? Find a neat compromise for me.
You said:
ok instead of that here is my idea:

so there is a chat interface which will welcome users with some welcome prompt. then, it will open up a pop up where the user can input the project details and everyone's githubs and resumes (with file upload or plain text upload). then, the user can click save and then all this info will be passed into gemini chat api. this chatbot will then call upon agents to parse resumes and parse github links, get the outputs and put it in a mongodb database. from the github, we will get languages used and then the readme from each repo per user, to get specific details about what has been done. the entries will then be embedded and vectorized. then, cosine similarity will be used to find people to delegate tasks to. then this info will be then sent back to gemini chat and it will give an output of people delegated to certain tasks. 

is this idea good for a hackathon? critique it heavily. is this just a gpt wrapper and does it seem pointless as a project? or, is there any value to doing this project? 
You said:
ok so give some concrete ways for me to fix this idea. 
You said:

**Task ‚Üí skills map** (curated JSON, not LLM):

Wouldn't an LLM be more accurate in terms of classifying skills found in the resume and github scrapes? I know that using an LLM makes it feel as if we haven't really done any work and just plugged in a ML model to do everything for us. But I'm thinking about how to make the project more accurate. It's more about the NUMBER of factors that we scrape right? I'm confused ‚Äî¬†where does the bulk of our project lie in, and how do we get started doing the work?

Nihal has worked on resume scraping, and Ruchi is working on github scraping. Yash gave up the Mongo effort. I need to figure out what I need to do personally too.
You said:
We have 2 peripherals/tools:
1. Resume Scraper: Utilizes Google Cloud OCR to scrape resume content and outputs neatly parsed JSON. Sometimes doesn't use OCR when it doesn't need to. But always outputs parsed JSON regardless of what form of resume file is uploaded.
2. Github Scraper: Scrapes github content and README's when necessary (but you'll need to help me understand how NOT to make useless calls to the API / temper how many calls I make to the API. I also want to scrape "most used languages" (language percent bar, essentially) altogether (for all projects) so I can get an idea of what languages the user is ACTUALLY proficient in, to complement what skills/languages are mentioned in their resume.

Now I'm confused as to what the CORE of my project might look like. I also need to know whether cosine similarity is used or not in the resume scraper and in the github scraper. Should cosine similarity be used to determine the prevalence of a skill/language used? I'm confused
You said:

* **Deterministic weighting:** map levels ‚Üí weights: advanced=1.0, intermediate=0.7, beginner=0.4.

Realistically how would you check if some skill is advanced? Would it be the number of times that specific term "python" is mentioned, or would it be the intensity of that term being mentioned (e.g. showing profound understanding of python and its nuances, even if it hasn't been mentioned more than once or twice), or some other way, or some compromise between the two? If it's a compromise, then I'm going to have to use some other utility to help me, since I believe it would be way too complicated to implement all of this in a 24-hour hackathon
You said:
For a Github project, how can I scrape the projects someone has collaborated on or contributed to, if I'm just given their github username? There's no real way to get this from their page. But those could be major contributions; for example, what if I work on a Github project that's not owned by me but by my lab leader?
You said:

Better path (GraphQL, one-shot and richer) ========================================== **What you‚Äôll get:** An official list of repos they contributed to (by commits, PRs, issues, reviews), with counts per repo. Query: graphql Copy code

How do I setup and run this code?
You said:

export GITHUB\_TOKEN=ghp\_your\_token\_here

Is this useful to use in a large-scale project where I'm scraping several people's githubs?
You said:

* Hackathon/small: one **Personal Access Token** (server-side only).

For a hackathon, I'm trying to keep one PAT. Would I still be able to scrape several different users with this one PAT using GraphQL?
You said:
Final Plan:
Start chat with Gemini chatbot with a rough project idea. Have a back and forth with the chatbot to improve the plan and come up with specific product specifications. 
After finishing the planning, click a button saying ‚ÄúDone Planning!‚Äù, which will launch the user into the next phase
The chatbot will call an agent to open a popup box for the user to input all of their teammates‚Äô GitHub profiles and resume pdfs. The user can then select ‚ÄúDone‚Äù. 
From the GitHub profile, get list of languages used using the API directly.
From both resume and GitHub profile, get list of key words. First get top n GitHub repos based on stars/recent push/most pushes, and use their READMEs. Then get the full resume text with OCR. 
Use Gemini to extract some key words from the resume and top READMEs and store in a list.
Get all the key words and languages used in a list. Add each list to MongoDB and vectorize with Atlas Vector Search. 
Use cosine similarity to delegate tasks to each person based on the finalized system requirements. 
Then return all the delegations by person to Gemini, which will return everything to the user in chat format. Then the user may continue to chat and ask for changes. 


SkillSync: AI Agents for Team Dynamics
 A multi-agent AI system that evaluates team members‚Äô technical and interpersonal attributes to suggest optimal role assignments, project planning, and collaboration structure.
Think of it as ‚ÄúAI for team optimization,‚Äù where intelligent agents combine data from professional sources (LinkedIn, GitHub, resumes) to form balanced teams or distribute responsibilities efficiently.

‚öôÔ∏è System Architecture
1. Data Collection Layer
Goal: Collect and parse data about each team member‚Äôs skills, experiences, and style.
Input Sources:


LinkedIn: Job roles, experience, endorsements, soft skills.


GitHub: Repo languages, contribution frequency, stars, recent projects.


Resume: Structured text input or PDF parsed using OCR + NLP.


Optional Personality Survey: Simple 5-question form or a Big Five personality quick test (e.g., open-source 16Personalities variant).


Technologies:


LangChain for text parsing pipelines


OpenAI API or LLaMA for skill extraction


BeautifulSoup or APIs (GitHub REST API) for data scraping


PyPDF2 or pdfminer.six for parsing resumes



2. AI Agent Layer
Each agent specializes in a unique analysis task. You can run them in parallel and make them communicate via a Controller Agent (like a multi-agent framework such as LangGraph, CrewAI, or Autogen).
a. üß† Skill Agent
Task: Extract and quantify technical expertise from GitHub, LinkedIn, or resumes.


Approach:


Use LLM-based text classification to detect skills.


Assign skill weights (e.g., Python: 0.9, React: 0.6).


Cluster related skills using embeddings + cosine similarity.


b. üßç Personality Agent
Task: Derive soft skills and working style (leader, planner, executor, creative).


Approach:


Use survey data or infer from profile text (e.g., ‚Äúmentored,‚Äù ‚Äúcollaborated,‚Äù ‚Üí team-oriented).


Map traits to Big Five dimensions (openness, conscientiousness, etc.).


Represent each member as a personality vector.


c. üîó Match Agent
Task: Recommend ideal team formations or member pairings.


Approach:


Use cosine similarity between skill sets for complementary matches.


Evaluate team diversity: combine technical diversity (skills) + personality diversity.


Output ‚Äúcompatibility score‚Äù for each pairing or group.


d. üìÖ Planner Agent
Task: Suggest optimal task-role assignments.


Approach:


Input: project task list (user-supplied or generated from description).


Map tasks to skill requirements (via LLM classification).


Match each task to the member with highest skill weight and interest.


Optionally, generate a Gantt-like task plan for project execution.



3. Coordination Layer (Multi-Agent Orchestration)
Use LangGraph or Autogen to connect agents:


Controller Agent coordinates info passing between agents.


Once Skill Agent and Personality Agent complete, Match Agent gets combined data.


Planner Agent uses final dataset to assign tasks.


Agents can communicate using shared memory (vector store) such as Pinecone, FAISS, or Chroma.



4. Frontend (User Interface)
Goal: Simple, modern dashboard for team input and results.


Tech Stack:


Frontend: Next.js / React + TailwindCSS


Backend: FastAPI / Flask


Database: MongoDB with vector storage


Features:


Upload resumes or link GitHub/LinkedIn URLs.


Display radar charts for skills and personality.


Visualize team compatibility using graph or heatmap.


‚ÄúAssign Tasks‚Äù button generates an AI task allocation plan.



üß† Example Workflow
Users upload their profiles or paste LinkedIn/GitHub links.


Skill Agent extracts skills (e.g., ‚ÄúYash: Python, ML, React‚Äù; ‚ÄúZayd: Java, Backend, DB‚Äù).


Personality Agent infers working style (‚ÄúYash: Analytical Leader‚Äù; ‚ÄúZayd: Logical Planner‚Äù).


Match Agent computes compatibility matrix (who works best with whom).


Planner Agent receives project goal (‚ÄúBuild AI trip planner‚Äù) and outputs task-role plan:

 Yash ‚Üí AI Module Lead  
Zayd ‚Üí Backend Integration  
Alice ‚Üí Frontend  
John ‚Üí Data Pipeline  
Dashboard displays team chart + recommendations.



Does this line up with what you're thinking of the project? I think we've removed the skill LEVEL feature and just kept scraping skills themselves as an MVP.
You said:
Final Plan (Revised)
1Ô∏è‚É£ Project Planning Phase
 Start a chat with the Gemini chatbot by describing a rough project idea. Engage in a back-and-forth conversation to refine the idea and generate detailed product specifications, including key features, components, and system requirements.
Once planning is complete, the user clicks ‚ÄúDone Planning!‚Äù, which launches the next phase.

2Ô∏è‚É£ Team Input Phase
 The chatbot calls an Agent that opens a popup form for the user to input all teammates‚Äô GitHub usernames and upload resume PDFs.
 After filling in the details, the user clicks ‚ÄúDone‚Äù to proceed.

3Ô∏è‚É£ Data Extraction Phase
 From each teammate‚Äôs GitHub profile, use the GitHub REST API to:
Retrieve a list of languages used across their repositories.
Identify the top N repositories (by recent activity or stars).
Fetch each top repo‚Äôs README and extract relevant technologies or project keywords.
From each resume, use OCR (for PDF parsing) to get full text.
 Then, use Gemini alongside a lightweight keyword dictionary to extract key technical skills and topics.
 This hybrid extraction ensures both LLM-driven context awareness and deterministic keyword coverage.

4Ô∏è‚É£ Feature Vectorization Phase
 Combine each person‚Äôs keywords and languages used into a single feature list.
 Store these features in Pinecone and vectorize them. 
Separately, take the finalized system requirements (from the planning chat) and embed them into a vector as well ‚Äî this becomes the query vector representing the project‚Äôs skill needs.

5Ô∏è‚É£ Matching & Task Delegation Phase
 Compute cosine similarity between the project query vector and each teammate‚Äôs feature vector.
 This produces a ranking of which teammates best match specific skill sets or project tasks.
Each assignment is returned with a short explanation ‚Äî for example:
‚ÄúAssigned Backend to Yash (0.82 match) ‚Äî strengths in Python, FastAPI, SQL, and recent API work.‚Äù

6Ô∏è‚É£ Chat Output & Iteration Phase
 The structured matching results are returned to Gemini, which formats them into a clear, conversational summary in the chat interface.
The user can continue chatting with Gemini to explore what-if scenarios, make role swaps, or add new teammates, with results dynamically recalculated.

üîß Technical Highlights
LLM Role: Gemini is used for idea refinement, keyword extraction, and conversational explanation ‚Äî not core logic.


Core Engine: Deterministic matching using cosine similarity on engineered feature vectors.


Explainability: Each assignment includes rationale and skill overlap percentages.


Resilience: Falls back to GitHub-only data if OCR or README extraction fails.


Scalable: MongoDB Atlas Vector Search stores embeddings; fallback in-memory vector store for quick testing.



This is my final plan again. I need you to analyze this significantly and compare it with what we have already discussed; i.e. the "avoiding a wrapper core" concept and skill level implementation. I will need you to synthesize what you already have in previous prompts and messages with what I have pasted in above, and write it in a more simple style that I can understand and break down as a beginner hackathon project developer.
You said:
Can you rewrite the "Final Plan" section in the exact format as what I pasted in? Above, you have just written what is new and what a day plan would look like, but I need the project flow itself. Can you write that, and just that, and make sure it is very detailed and specific in terms of what we need?
You said:
I'm so confused. Can you dumb it down more for me like the text I pasted in?